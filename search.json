[{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530124200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530124200,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://amr4i.github.io/privacy/","publishdate":"2018-06-28T00:00:00+05:30","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":" Location This project was done as a research internship at Big Data Experience Lab, Adobe Systems, Bangalore during May \u0026lsquo;18 - Jul \u0026lsquo;18, the summer of my third year.\nDetails During the internship, I worked as a Research Intern, among a team of 3 members and in close coordination with multiple research members of technical staff at the company, to accomplish the above-mentioned project. A general area of interest was assigned, and a specific problem statement was chosen in that area after an extensive period of brainstorming. Having decided on a problem statement, methods were devised to come up with a feasible solution for the problem. Multiple NLP techniques such as scene graph parsing, coreference resolution, and others were utilised to understand text, along with learning multiple MLP networks for certain tasks involved in the solution pipeline. The augmented reality aspect of the project was realized by using the ARCore library provided by Google, in the Unity3D framework.\nThe final solution pipeline included multiple methods leveraged from existing works and improved upon, as well as novel methods for scene augmentation and a MLP prediction model that were introduced for the first time.\nUsing these multiple techniques intelligently in a pipelined manner, a first-of-its-kind novel end-to-end interface was developed that allows easy authoring of augmented reality experiences from natural language input. An internal survey was conducted, which temporarily validated the genuinity of the interface, and conducting the final crowdsourced survey is work in progress.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"41449180bcd520ef3e0e053b1e100795","permalink":"https://amr4i.github.io/project/augmented-reality/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/augmented-reality/","section":"project","summary":"Developed a novel end-to-end interface that allows easy authoring of Augmented Reality experiences from natural language input, allowing visualization of any text in Augmented Reality. Patent is under consideration.","tags":["industrial","research","augmented-reality","machine-learning","nlp"],"title":"Augmented Reality Authoring","type":"project"},{"authors":null,"categories":null,"content":" Details This project was done as my first undergraduate research project under Prof. Medha Atre, Department of Computer Science and Engineering, IIT Kanpur.\nAbstract This project aims to utilize the the emotional information present in any media to perform cross-modal media retrieval efficiently. We present an implementation to extract this emotion from images as 2-dimensional vectors, and propose two methods to bring this emotion vectors from different medias in the same space, one being a statistical approach, and the other being a learning based approach. We also present a hypothesis, which allows us to establish a ground truth for the cross modal mapping. We perform extensive analysis of one of our proposals, and report the results obtained. We have also proposed and implemented a heuristic to retrieve cross-modal results efficiently. The details of our project can be found in the project report here.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"3a2ab6dbcbdd239308db7ace7bfb1b78","permalink":"https://amr4i.github.io/project/cross-modal-media/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/cross-modal-media/","section":"project","summary":"Aimed at utilising the emotional information present in images and audios to perform cross modal media retrieval. We propsed a hypothesis, and two methods, implemented them, and performed experiments to verify our claims.","tags":["research","information-retrieval","machine-learning","computer-vision","big-data"],"title":"Cross Modal Media Retrieval","type":"project"},{"authors":null,"categories":null,"content":" Location This project was done as a industrial software internship at Hike Pvt. Ltd., New Delhi, during May \u0026lsquo;17 - Jul \u0026lsquo;17, the summer of my second year.\nDetails The project aimed at developing a machine learning model to assign an emotion to any chat message provided as input. This would allow quick sticker recommendations to be based not only the textual reference, but also take into account the emotional state of the user for that message.\nThis task was accomplished by developing neural network models using the open source software library tensorflow from Google. In order to get the highest accuracy, multiple models had to be developed, all based on different machine learning techniques including CNN, RNN, and LSTM networks.\nThe esteem of the model lies in the fact that it is a language independent model, which when trained on enough texts from any language, should be able to successfully predict the most probable emotion of any text in that language.\nThe project also comprised of gathering the data required for training this model. For this purpose, a sub-project had to be developed that acted like a Language-Classifier, which segregated the chat corpus into chats of the top 5 chat languages in the application, namely Hindi+English, Tamil, Marathi, Gujarati and Bengali, but can easily be extended to any number of languages as required.\nThis segregation into the different chatting languages was required since the data needed for training had to be prepared by manual tagging of the messages to a particular emotion class out of a predefined set of classes. Notably, the model does not require any language classification of the input message, once it has been trained on this prepared data.\nThe crude conversation data was obtained from a corpus of anonymized \u0026ldquo;random\u0026rdquo; text chat sequences. Following this, the language-classifier was applied to get the texts from each language separated out. The classifier started with the sticker tags of the languages, processed them at multiple stages and performed a frequency analysis through a Confusion Matrix to get unique set of words for each language. Using these words, some texts were extracted from the corpus for each language. These texts were then used to get a better and more extensive list of unique common words through the same process. Now, using this final word list, another frequency based search on the conversations allowed extraction of texts uniquely belonging to each language.\nThese texts then had to be properly “cleaned\u0026rdquo; so as to contain only relevant information. This was done using some basic python scripting followed by running the corpus through a Vector Space Model (VSM) that used word2Vec modelling and tf-idf weights to generate word and document vectors, and use those to clear irrelevant and repeated information. The process also involved gathering data from multiple NoSQL databases in MongoDB.\nFinally, a Server-Client support for the model was also developed wherein the user could submit the text, which is processed by the Client and submitted to the Server along the necessary details as a GRPC request. The server hosts the multiple versions of the model, with the latest being fetched to answer the query it receives from the client. This support was added through Google\u0026rsquo;s tensorflow_serving library. The model can also be hosted on Google Machine Learning Engine, the support for which can be easily implemented in the existing code.\nThe tensorflow serving library has support only on linux environments, and hence for setting up the server system, a appropriate Docker environment also had to be set-up on the local machine, that included all the dependencies for the same.\nAs a simple but efficient extension, the text message that contains emojis isn\u0026rsquo;t analyzed via the model, and instead the emotion can be directly taken from the emoji itself on the client itself without any need to hit the server.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"dd426e425d0565677a9d56e446eca7a5","permalink":"https://amr4i.github.io/project/language-independent-emotion/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/language-independent-emotion/","section":"project","summary":"The project aimed at developing a machine learning model to assign an emotion to any chat message provided as input, in any of the top five languages common in Indian chat scenarios, although in the Latin Script.","tags":["industrial","machine-learning","nlp","big-data"],"title":"Language Independent Text-to-Emotion Classification","type":"project"},{"authors":null,"categories":null,"content":" Location This project was done as a remote software development internship at New York Office, IIT Kanpur during May \u0026lsquo;18 - Jul \u0026lsquo;18, the summer of my second year.\nDetails The project comprised of completing multiple tasks within the assigned duration of the internship. The first task was to implement the Document Distance problem, to remove the semantically same articles from large collection of articles. This was accomplished by using word vectors and document vectors and implementing the Word Mover’s Distance algorithm to successfully eliminate all similar entries within acceptable error values. We also experimented with word-vectors and tf-idf weights to generate document vectors and finding semantically similar articles through cosine similarity of these document vectors, but Word Mover’s distance was the final method used because of better accuracy, though being slower than the other approach. The development was done in Python, and used both pre-trained word embeddings and custom trained word embeddings to get a requirement specific model.\nThe second task was to implement the Reverse k-Nearest Neighbour problem. The data would contain two sets of coordinates, one for facilities and the other for users. The task is to return all those users (BiChromatic RkNN) or facilities(MonoChromatic RkNN) for which the query facility is among the k-Nearest facility. This task required proper data structure that could allow quick access and insertion along with efficient spatial storage, as we need to work with 2D coordinates. Hence, two separate R-Trees were used to store the coordinates of users and facilities. Then the SLICE algorithm was used to implement RkNN problem, that partitions the space in equal sectors around the query, and acts as an improved version of halfSpace and Six-region algorithms combined to get pruning and verification done relatively faster in speed and I/O operations combined. The code for this problem was implemented in C++.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"2530d2d3581dd503984b528e88cc1d1f","permalink":"https://amr4i.github.io/project/nyo/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/nyo/","section":"project","summary":"Implemented the Document Distance problem using the Word Mover's Distance, and the reverse k-NN problem, using the SLICE algorithm.","tags":["industrial","big-data"],"title":"Machine Learning for large scale logistics platform","type":"project"},{"authors":null,"categories":null,"content":" Details This project was done as part of the course CS671: Introduction to Natural Language Processing, in the Spring \u0026lsquo;18 term at IIT Kanpur under Prof. Harish Karnick, Department of Computer Science and Engineering, IIT Kanpur.\nAbstract Much work has been done on Neural Machine Translation, but most of it requires the availability of a large parallel corpora to train on, which is not always readily available. Unsupervised methods also exist that work by encoding both the languages in consideration to a common latent space, and then decoding them to the target language as required. This work aims at an accurate implementation of the work presented in Lample et al. (2017), and train it on a non-parallel corpora of English and Catalan languages. Baseline implementation is evaluated using BLEU score metric. We also hypothesize that adding the GCN layers would optimize the existing representations\nThe details of our project can be found in the project report here.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"690c421333638a563b731ec5a064a73c","permalink":"https://amr4i.github.io/project/neural-machine-translation/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/neural-machine-translation/","section":"project","summary":"Performed an extensive literature review on the existing methods of Neural Machine Translation using parallel and non-parallel corpora. Implemented the state-of-the-art work using non-parallel corpora on low resource language Catalan and performed analysis of results obtained to explain the possible differences arising on using a low resource language.","tags":["machine-learning","nlp"],"title":"Neural Machine Translation for Low Resource Languages","type":"project"},{"authors":null,"categories":null,"content":" Details This project was done as research project in the course CS682: Quantum Computing, in the Fall \u0026lsquo;17 term at IIT Kanpur under Prof. Rajat Mittal, Department of Computer Science and Engineering, IIT Kanpur.\nAbstract The aim of the project is to study two of the most widely used machine learning strategies, namely KNearest Neighbours algorithm and Perceptron Learning algorithm, in a quantum setting, and study the speedups that the quantum modules allow over the classical counterparts.\nThe study is primarily based on the following 3 papers:\n Quantum Perceptron Models, by N. Wiebe, A. Kapoor and K. M. Svore. Quantum Algorithm for K-Nearest Neighbors Classification Based on the Metric of Hamming Distance, by Y. Ruan, X. Xue, H. Liu, J. Tan, and X. Li. Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning, by N. Wiebe, A. Kapoor and K. M. Svore.  The details of our project can be found in the project report here.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"158b545ad437486e340094c952dcdf8d","permalink":"https://amr4i.github.io/project/quantum-machine-learning/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/quantum-machine-learning/","section":"project","summary":" Studied two of the most widely used machine learning strategies, namely K-Nearest Neighbours algorithm and Perceptron Learning algorithm, in a quantum setting, and study the speedups that the quantum modules allow over the classical counterparts.","tags":["research","theory"],"title":"Quantum Machine Learning","type":"project"},{"authors":null,"categories":null,"content":" Details This project was started as part of the course CS657: Information Retrieval, in the Spring \u0026lsquo;18 term at IIT Kanpur under Prof. Arnab Bhattacharya, Department of Computer Science and Engineering, IIT Kanpur. It has been continued beyond the course into my second undergraduate research project, and is still in progress.\nAbstract In this project, we introduced a novel pipeline to implement a query-biased multi-document abstractive summarisation. Traditional information retrieval systems return a ranked list of whole documents as the answer to a query. However, in many cases, not every part of an entire document is relevant to the query. Thus, it is desirable to retrieve from the set of retrieved documents, in a succinct manner, a summary of only the required information extracted from across all the relevant documents. The approach proposed involves retrieving relevant documents for each query, followed by extracting relevant passages from each document. This is followed by collating all such relevant passages and performing redundancy removal to keep the length of such a collated document sane. Finally, an abstractive summarisation approach is used to generate abstractive single-line summaries for our information need.\nThe details of our project, as of now, can be found in the project report here.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"1514c0d6945001e9efc8aa8374231935","permalink":"https://amr4i.github.io/project/query-abs-summary/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/query-abs-summary/","section":"project","summary":"Implemented multiple modifications and techniques for improving the state-of-the-art TPMS system. Proposed and implemented an alternating optimization approach for completing the matrix of relevance scores between paper vectors and author vectors. Still in progress.","tags":["research","information-retrieval","nlp","machine-learning","big-data"],"title":"Query Biased Multi-Dcoument Abstractive Summarisation","type":"project"},{"authors":null,"categories":null,"content":" Details This project was done as part of the course CS771: Machine Learning Techniques, in the Fall \u0026lsquo;17 term at IIT Kanpur under Prof. Purushottam Kar, Department of Computer Science and Engineering, IIT Kanpur.\nAbstract The process of reviewer assignment to the papers of a conference is a very challenging and sensitive task. The choice of reviewers for a particular paper plays a crucial role in determining whether the paper is accepted into the conference. Automating the task of choosing reviewers is one of the recent challenges being actively researched by the machine learning community. The state of the art paper-matching system is the Toronto Paper Matching System which uses an LDA based topic modelling approach to identify topics in the entire paper and a simple dot product to assess similarity between reviewer topics and paper topics. Our approach builds on this line of work and develops an alternating optimization approach for completing the matrix of relevance scores between paper vectors and author vectors.\nWe did an extensive survey of the feild and understood the currently most prevalent techniques for automated paper-reviewer assignment like the Toronto Paper Matching System, and the Robust Paper-Reviewer Assignment Model. We implemented multiple modifications and techniques for improving the TPMS system by improvising on the Latent Dirichlet Allocation technique used in the generative model, and adding some intuitive biases. We also implemented an alternating optimization approach for completing the matrix of relevance scores between paper vectors and author vectors.\nThe details of our project can be found in the project report here.\n","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"f4d9f717fed0a4510983e50ff8dc948a","permalink":"https://amr4i.github.io/project/reviewer-recommendation/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/reviewer-recommendation/","section":"project","summary":"Implemented multiple modifications and techniques for improving the state-of-the-art TPMS system. Proposed and implemented an alternating optimization approach for completing the matrix of relevance scores between paper vectors and author vectors.","tags":["research","machine-learning","nlp"],"title":"Reviewer Recommendation for Conference Paper Submissions","type":"project"}]